{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1615276479856
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### CMPE 256 Class Presentation - VAR Commodity Pricing Model - Gold & Silver ###\n",
    "###### Covers: Book Topics: Machine Learning and Artificial Intelligence for Agricultural Economics: Prognostic Data Analytics to Serve Small Scale Farmers Worldwide, Publisher :  Springer; 1st ed. 2021 edition (October 5, 2021), ISBN-13: 978-3030774844 ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6\n",
    "\n",
    "***FORECASTING of Gold and Oil*** have garnered major attention from academics, investors and Government agencies like. These two products are known for their substantial influence on global economy. I will show here, how to use Granger’s Causality Test to test the relationships of multiple variables in the time series and Vector Auto Regressive Model (VAR) to forecast the future Gold & Oil prices from the historical data of ***Gold prices, Silver prices, Crude Oil prices, Stock index , Interest Rate and USD rate***.\n",
    "\n",
    "https://www.lbma.org.uk/prices-and-data/precious-metal-prices#/\n",
    "https://fred.stlouisfed.org/series/FEDFUNDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random \n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy.stats as spstats\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.reload_library()\n",
    "mpl.style.use('classic')\n",
    "mpl.rcParams['figure.facecolor'] = (1, 1, 1, 0)\n",
    "mpl.rcParams['figure.figsize'] = [6.0, 4.0]\n",
    "mpl.rcParams['figure.dpi'] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<H1> Gold Dataset </H1> <H3> Load Glod Dataset and set Date as Index. </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276484359
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Gold price data\n",
    "goldDF = pd.read_csv(\"lbma_gold_am_usd_1967-12-31_2022-03-31.csv\",parse_dates=['Date'], dayfirst=True)\n",
    "goldDF = goldDF.set_index('Date')\n",
    "goldDF.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<h3> Check for nulls </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276487563
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldDF.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276491183
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276499335
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276502163
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "fig, ax = plt.subplots()\n",
    "goldDF['GoldPrice'].hist(color='#A9C5D3', edgecolor='black',  \n",
    "                          grid=False)\n",
    "ax.set_title('Gold Price', fontsize=12)\n",
    "ax.set_xlabel('Gold Price', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<h5> Create Gold Price Bins - divide price into five quantiles (0, 25%, 50%, 75%, and 100%) </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276506953
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldDF['GoldPrice_bin_round'] = np.array(np.floor(\n",
    "                              np.array(goldDF['GoldPrice']) / 10.))\n",
    "goldDF[['GoldPrice', 'GoldPrice_bin_round']].iloc[1071:2076]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276510116
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "quantile_list = [0, .25, .5, .75, 1.]\n",
    "quantiles = goldDF['GoldPrice'].quantile(quantile_list)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276513075
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "goldDF['GoldPrice'].hist(bins=30, color='#A9C5D3', \n",
    "                             edgecolor='black', grid=False)\n",
    "for quantile in quantiles:\n",
    "    qvl = plt.axvline(quantile, color='r')\n",
    "ax.legend([qvl], ['Quantiles'], fontsize=10)\n",
    "ax.set_title('Gold Price Histogram with Quantiles', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Gold Price', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276516765
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goldDF['GoldPrice_log'] = np.log((1+ goldDF['GoldPrice']))\n",
    "goldDF[[ 'GoldPrice', 'GoldPrice_log']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276520201
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldprice_log_mean = np.round(np.mean(goldDF['GoldPrice_log']), 2)\n",
    "fig, ax = plt.subplots()\n",
    "goldDF['GoldPrice_log'].hist(bins=30, color='#A9C5D3', \n",
    "                                 edgecolor='black', grid=False)\n",
    "plt.axvline(goldprice_log_mean, color='r')\n",
    "ax.set_title('Gold Price Histogram after Log Transform', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Gold Price (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.text(11.5, 450, r'$\\mu$='+str(goldprice_log_mean), fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276523327
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldprice = np.array(goldDF['GoldPrice'])\n",
    "goldprice_clean = goldprice[~np.isnan(goldprice)]\n",
    "l, opt_lambda = spstats.boxcox(goldprice_clean)\n",
    "print('Optimal lambda value:', opt_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276526424
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldDF['goldprice_boxcox_lambda_0'] = spstats.boxcox(\n",
    "                                        (1+goldDF['GoldPrice']), \n",
    "                                          lmbda=0)\n",
    "goldDF['goldprice_boxcox_lambda_opt'] = spstats.boxcox(\n",
    "                                            goldDF['GoldPrice'], \n",
    "                                              lmbda=opt_lambda)\n",
    "\n",
    "goldDF[['GoldPrice', 'GoldPrice_log', \n",
    "               'goldprice_boxcox_lambda_0',       \n",
    "               'goldprice_boxcox_lambda_opt']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276529508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "goldprice_boxcox_mean = np.round(\n",
    "                      np.mean(\n",
    "                       goldDF['goldprice_boxcox_lambda_opt']),2)\n",
    "fig, ax = plt.subplots()\n",
    "goldDF['goldprice_boxcox_lambda_opt'].hist(bins=30, \n",
    "                     color='#A9C5D3', edgecolor='black', grid=False)\n",
    "plt.axvline(goldprice_boxcox_mean, color='r')\n",
    "ax.set_title('Gold Price after Box–Cox Transform', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Gold Price (Box–Cox transform)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.text(24, 450, r'$\\mu$='+str(goldprice_boxcox_mean), fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276533694
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Silver price data\n",
    "silverDF = pd.read_csv(\"lbma_silver_am_usd_1967-12-31_2022-03-31.csv\",parse_dates=['Date'], dayfirst=True)\n",
    "silverDF = silverDF.set_index('Date')\n",
    "silverDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276536012
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "silverDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276538901
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "silverDF.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276542303
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "mean_imputation_silverDF = silverDF.copy()\n",
    "mean_imputation_silverDF['SilverPrice_Mean_Filled'] = mean_imputation_silverDF['SilverPrice'].fillna(silverDF['SilverPrice'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276545332
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "silverDF.SilverPrice.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276549653
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "mean_imputation_silverDF.SilverPrice.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276553448
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sns.displot(silverDF, x=\"SilverPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276558143
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sns.displot(mean_imputation_silverDF, x=\"SilverPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silverDF['SilverPrice_log'] = np.log((1+ silverDF['SilverPrice']))\n",
    "silverDF[[ 'SilverPrice', 'SilverPrice_log']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "silverprice_log_mean = np.round(np.mean(silverDF['SilverPrice_log']), 2)\n",
    "fig, ax = plt.subplots()\n",
    "silverDF['SilverPrice_log'].hist(bins=30, color='#A9C5D3', \n",
    "                                 edgecolor='black', grid=False)\n",
    "plt.axvline(silverprice_log_mean, color='r')\n",
    "ax.set_title('Silver Price Histogram after Log Transform', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Silver Price (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.text(11.5, 450, r'$\\mu$='+str(silverprice_log_mean), fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "! pip install impyute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276562969
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# silverDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276567942
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# USD Index\n",
    "# Trade Weighted U.S. Dollar Index: Broad, Goods and Services (DTWEXBGS)\n",
    "# https://fred.stlouisfed.org/series/DTWEXBGS \n",
    "usdFedFundsDF = pd.read_csv(\"FEDFUNDS_2022-03-31.csv\",parse_dates=['Date'], dayfirst=True)\n",
    "\n",
    "\n",
    "usdFedFundsDF = usdFedFundsDF.set_index('Date')\n",
    "\n",
    "usdFedFundsDF.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdFedFundsDF.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276571729
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "usdFedFundsDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276589261
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "usdFedFundsDF.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276593046
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Interest\n",
    "# 10-Year Treasury Constant Maturity Rate (DGS10)\n",
    "# https://fred.stlouisfed.org/series/DGS10\n",
    "\n",
    "### https://fred.stlouisfed.org/series/FEDFUNDS\n",
    "interestRateDF = pd.read_csv(\"DGS10.csv\",parse_dates=['DATE'], dayfirst=True)\n",
    "interestRateDF = interestRateDF.rename(columns={\"DATE\":'Date'})\n",
    "\n",
    "interestRateDF = interestRateDF.set_index('Date')\n",
    "\n",
    "interestRateDF.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestRateDF.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestRateDF['DGS10'] = interestRateDF['DGS10'].replace('.',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestRateDF.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276595437
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "interestRateDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276597923
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "interestRateDF.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276603598
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# S&P Index\n",
    "# https://finance.yahoo.com/quote/%5EGSPC/history?period1=-1325635200&period2=1611360000&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true\n",
    "# https://finance.yahoo.com/quote/%5EGSPC/history?period1=-628819200&period2=1650412800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true\n",
    "spindex500Index = pd.read_csv(\"SPIndex_2022-03-31.csv\",parse_dates=['Date'], dayfirst=True)\n",
    "\n",
    "spindex500Index = spindex500Index.set_index('Date')\n",
    "\n",
    "spindex500Index.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276604807
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spindex500Index.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spindex500Index.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276606970
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spindex500Index = spindex500Index.drop('Open', 1)\n",
    "spindex500Index = spindex500Index.drop('High', 1)\n",
    "spindex500Index = spindex500Index.drop('Low', 1)\n",
    "spindex500Index = spindex500Index.drop('Close*', 1)\n",
    "spindex500Index = spindex500Index.drop('Volume', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "spindex500Index= spindex500Index.rename(columns={'Adj Close**':'AdjClose'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spindex500Index['AdjClose'] = spindex500Index['AdjClose'].replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276609388
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spindex500Index.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spindex500Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spindex500Index=spindex500Index.loc['19720101':'20220419']\n",
    "#spindex500Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276612835
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Crude Oil Prices: West Texas Intermediate (WTI) - Cushing, Oklahoma\n",
    "# https://fred.stlouisfed.org/series/DCOILWTICO\n",
    "oilPricesDF = pd.read_csv(\"DCOILWTICO_2022-03-31.csv\",parse_dates=['Date'], dayfirst=True)\n",
    "oilPricesDF = oilPricesDF.set_index('Date')\n",
    "oilPricesDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oilPricesDF['CrudeOilPrices(WTI)'] = oilPricesDF['CrudeOilPrices(WTI)'].replace('.',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276617342
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "oilPricesDF.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oilPricesDF['CrudeOilPrices(WTI)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis: ###\n",
    "\n",
    "Let’s load the data and do some analysis with visualization to know insights of the data. Exploratory data analysis is quite extensive in multivariate time series. I will cover some areas here to get insights of the data. However, it is advisable to conduct all statistical tests to ensure our clear understanding on data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276620146
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "oilPricesDF.columns\n",
    "### https://stackoverflow.com/questions/46834732/convert-pandas-datetime-column-yyyy-mm-dd-to-yyyymmdd\n",
    "## https://datatofish.com/strings-to-datetime-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276624254
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "dataset = pd.concat([goldDF,silverDF,oilPricesDF,usdFedFundsDF,interestRateDF,spindex500Index], axis=1)\n",
    "print('Number of colums in Dataframe : ', len(dataset.columns))\n",
    "print('Number of rows in Dataframe : ', len(dataset.index))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276759364
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276756845
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.drop(['GoldPrice_bin_round','GoldPrice_log','goldprice_boxcox_lambda_0','goldprice_boxcox_lambda_opt','SilverPrice_log'], axis = 1, inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let’s fix the dates for all the series.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276762730
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "### dataset=dataset.loc['20100101':'20200824']\n",
    "dataset=dataset.loc['20000101':'20220301']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276766578
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276769281
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31170550/pandas-df-fillnamethod-pad-not-working-on-28000-row-df\n",
    "\n",
    "dataset=dataset.fillna(method='pad') \n",
    "dataset = dataset.fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum() ## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276772531
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset=dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276776958
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276790995
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, dpi=120, figsize=(10,6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    " data = dataset[dataset.columns[i]]\n",
    " ax.plot(data, color='red', linewidth=1)\n",
    " ax.set_title(dataset.columns[i])\n",
    " ax.xaxis.set_ticks_position('none')\n",
    " ax.yaxis.set_ticks_position('none')\n",
    " ax.spines[\"top\"].set_alpha(0)\n",
    " ax.tick_params(labelsize=6)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***From the above plots, we can visible conclude that, all the series contain unit root with stochastic trend showing a systematic pattern that is unpredictable.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Test ###\n",
    "To extract maximum information from our data, it is important to have a ***normal or Gaussian distribution*** of the data. \n",
    "To check for that, we have done a normality test based on the Null and Alternate Hypothesis intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemcost='1,455.22'\n",
    "itemcostProc=itemcost.replace(',','')\n",
    "print(itemcostProc)\n",
    "print(float(itemcostProc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['AdjClose']= dataset['AdjClose'].apply(lambda x: x.replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['AdjClose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CrudeOilPrices(WTI)'] = dataset['CrudeOilPrices(WTI)'].astype('float64')\n",
    "dataset['CrudeOilPrices(WTI)'] = dataset['CrudeOilPrices(WTI)'].astype('float64')\n",
    "dataset['DGS10'] = dataset['DGS10'].astype('float64')\n",
    "dataset['AdjClose'] = dataset['AdjClose'].astype('float64')\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CrudeOilPrices(WTI)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276793004
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "GoldPrice=dataset.GoldPrice.values\n",
    "print(GoldPrice)\n",
    "\n",
    "\n",
    "stat,p = stats.normaltest(GoldPrice)\n",
    "\n",
    "print(\"GoldPrice Statistics = %.3f, p=%.3f\" % (stat,p))\n",
    "alpha = 0.05\n",
    "if p> alpha:\n",
    "    print('Data looks Gaussian (fail to reject null hypothesis)')\n",
    "else:\n",
    "    print('Data looks non-Gaussian (reject null hypothesis)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276797998
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "SilverPrice=dataset.SilverPrice.values\n",
    "print(SilverPrice)\n",
    "\n",
    "\n",
    "stat,p = stats.normaltest(SilverPrice)\n",
    "\n",
    "print(\"SilverPrice Statistics = %.3f, p=%.3f\" % (stat,p))\n",
    "alpha = 0.05\n",
    "if p> alpha:\n",
    "    print('Data looks Gaussian (fail to reject null hypothesis)')\n",
    "else:\n",
    "    print('Data looks non-Gaussian (reject null hypothesis)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276801074
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "OilPrice=dataset['CrudeOilPrices(WTI)'].values\n",
    "print(OilPrice)\n",
    "\n",
    "\n",
    "stat,p = stats.normaltest(OilPrice)\n",
    "\n",
    "print(\"OilPrice Statistics = %.3f, p=%.3f\" % (stat,p))\n",
    "alpha = 0.05\n",
    "if p> alpha:\n",
    "    print('Data looks Gaussian (fail to reject null hypothesis)')\n",
    "else:\n",
    "    print('Data looks non-Gaussian (reject null hypothesis)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276804077
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "FEDFUNDS=dataset.FEDFUNDS.values\n",
    "print(FEDFUNDS)\n",
    "\n",
    "\n",
    "stat,p = stats.normaltest(FEDFUNDS)\n",
    "\n",
    "print(\"FEDFUNDS Statistics = %.3f, p=%.3f\" % (stat,p))\n",
    "alpha = 0.05\n",
    "if p> alpha:\n",
    "    print('Data looks Gaussian (fail to reject null hypothesis)')\n",
    "else:\n",
    "    print('Data looks non-Gaussian (reject null hypothesis)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276806584
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "InterestRate=dataset.DGS10.values\n",
    "print(InterestRate)\n",
    "\n",
    "\n",
    "stat,p = stats.normaltest(InterestRate)\n",
    "\n",
    "print(\"InterestRate Statistics = %.3f, p=%.3f\" % (stat,p))\n",
    "alpha = 0.05\n",
    "if p> alpha:\n",
    "    print('Data looks Gaussian (fail to reject null hypothesis)')\n",
    "else:\n",
    "    print('Data looks non-Gaussian (reject null hypothesis)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615314981455
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_data = dataset.resample('W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = dataset.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# Takes long time \n",
    "# fig, axes = plt.subplots(nrows=3, ncols=2, dpi=120, figsize=(10,6))\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#  data = monthly_data[monthly_data.columns[i]]\n",
    "#  ax.plot(data, color='red', linewidth=1)\n",
    "#  ax.set_title(monthly_data.columns[i])\n",
    "#  ax.xaxis.set_ticks_position('none')\n",
    "#  ax.yaxis.set_ticks_position('none')\n",
    "#  ax.spines[\"top\"].set_alpha(0)\n",
    "#  ax.tick_params(labelsize=6)\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data = dataset.resample('Y').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# Takes long time \n",
    "# fig, axes = plt.subplots(nrows=3, ncols=2, dpi=120, figsize=(10,6))\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#  data = yearly_data[yearly_data.columns[i]]\n",
    "#  ax.plot(data, color='red', linewidth=1)\n",
    "#  ax.set_title(yearly_data.columns[i])\n",
    "#  ax.xaxis.set_ticks_position('none')\n",
    "#  ax.yaxis.set_ticks_position('none')\n",
    "#  ax.spines[\"top\"].set_alpha(0)\n",
    "#  ax.tick_params(labelsize=6)\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615313554669
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "dataset.GoldPrice.plot(kind = 'density')  \n",
    "\n",
    "print('Gold: Kurtosis of normal distribution: {}'.format(stats.kurtosis(dataset.GoldPrice)))\n",
    "print('Gold:Skewness of normal distribution: {}'.format(stats.skew(dataset.GoldPrice)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions give us some intuition about the distribution of our data. The kurtosis of this dataset is -0.18. Since this value is less than 0, it is considered to be a light-tailed dataset. It has as much data in each tail as it does in the peak. Moderate skewness refers to the value between -1 and -0.5 or 0.5 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615313791303
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "dataset.SilverPrice.plot(kind = 'density') \n",
    "\n",
    "print('Silver: Kurtosis of normal distribution: {}'.format(stats.kurtosis(dataset.SilverPrice)))\n",
    "print('Silver:Skewness of normal distribution: {}'.format(stats.skew(dataset.SilverPrice)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions give us some intuition about the distribution of our data. The kurtosis of this dataset is -0.95. Since this value is less than 0, it is considered to be a light-tailed dataset. It has as much data in each tail as it does in the peak. Moderate skewness refers to the value between -1 and -0.5 or 0.5 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615313836563
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "dataset['CrudeOilPrices(WTI)'].plot(kind = 'density') \n",
    "\n",
    "print('Oil: Kurtosis of normal distribution: {}'.format(stats.kurtosis(dataset['CrudeOilPrices(WTI)'])))\n",
    "print('Oil:Skewness of normal distribution: {}'.format(stats.skew(dataset['CrudeOilPrices(WTI)'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions give us some intuition about the distribution of our data. The kurtosis of this dataset is -1.16. Since this value is less than 0, it is considered to be a light-tailed dataset. It has as much data in each tail as it does in the peak. Moderate skewness refers to the value between -1 and -0.5 or 0.5 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615313875635
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "dataset.FEDFUNDS.plot(kind = 'density') \n",
    "\n",
    "print('FEDFUNDS: Kurtosis of normal distribution: {}'.format(stats.kurtosis(dataset.FEDFUNDS)))\n",
    "print('FEDFUNDS:Skewness of normal distribution: {}'.format(stats.skew(dataset.FEDFUNDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions give us some intuition about the distribution of our data. The kurtosis of this dataset is -1.43. Since this value is less than 0, it is considered to be a light-tailed dataset. It has as much data in each tail as it does in the peak. Moderate skewness refers to the value between -1 and -0.5 or 0.5 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276826375
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "dataset['GoldPrice'].hist(bins=50)\n",
    "plt.title('Gold')\n",
    "plt.subplot(1,2,2)\n",
    "stats.probplot(dataset['GoldPrice'], plot=plt);\n",
    "dataset.GoldPrice.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal probability plot also shows the data is far from normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276831353
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "dataset['SilverPrice'].hist(bins=50)\n",
    "plt.title('Silver')\n",
    "plt.subplot(1,2,2)\n",
    "stats.probplot(dataset['SilverPrice'], plot=plt);\n",
    "dataset.SilverPrice.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal probability plot also shows the data is far from normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276834957
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "corr = dataset.corr()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(corr,xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True,annot_kws={'size':12})\n",
    "heat_map=plt.gcf()\n",
    "heat_map.set_size_inches(10,6)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276840305
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-correlation ###\n",
    "\n",
    "###### Auto-correlation or serial correlation can be a significant problem in analyzing historical data if we do not know how to look out for it. #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276845073
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# plots the autocorrelation plots for each stock's price at 50 lags\n",
    "for i in dataset:\n",
    "    series = dataset[i]\n",
    "    sm.graphics.tsa.plot_acf(series)\n",
    "    plt.title('ACF for %s' % i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276852049
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# plots the autocorrelation plots for each stock's price at 50 lags\n",
    "for i in dataset:\n",
    "    sm.graphics.tsa.plot_acf(dataset[i], lags = 50)\n",
    "    plt.title('ACF for %s' % i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here from the above plots, the auto-correlation of +1 which represents a perfect positive correlation which means, an increase seen in one time series leads to a proportionate increase in the other time series. We definitely need to apply transformation and neutralize this to make the series stationary. It measures linear relationships; even if the auto-correlation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276857815
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['AdjClose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Data: ###\n",
    "The VAR model will be fitted on X_train and then used to forecast the next 15 observations. These forecasts will be compared against the actual present in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276866746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "n_obs=15\n",
    "X_train, X_test = dataset[0:-n_obs], dataset[-n_obs:]\n",
    "print(X_train.shape, X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation: ###\n",
    "\n",
    "Applying first differencing on training set to make all the series stationary. However, this is an iterative process where we after first differencing, the series may still be non-stationary. We shall have to apply second difference or log transformation to standardize the series in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276869768
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "transform_data = X_train.diff().dropna()\n",
    "transform_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276872691
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "transform_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity check ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276877540
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as sm\n",
    "def augmented_dickey_fuller_statistics(time_series):\n",
    "    result = sm.adfuller(time_series.values, autolag='AIC')\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276881651
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print('Augmented Dickey-Fuller Test: Gold Price Time Series')\n",
    "augmented_dickey_fuller_statistics(transform_data['GoldPrice'])\n",
    "print('Augmented Dickey-Fuller Test: Silver Price Time Series')\n",
    "augmented_dickey_fuller_statistics(transform_data['SilverPrice'])\n",
    "print('Augmented Dickey-Fuller Test: CrudeOilPrices(WTI)  Time Series')\n",
    "augmented_dickey_fuller_statistics(transform_data['CrudeOilPrices(WTI)'])\n",
    "print('Augmented Dickey-Fuller Test: FEDFUNDS Time Series')\n",
    "augmented_dickey_fuller_statistics(transform_data['FEDFUNDS'])\n",
    "print('Augmented Dickey-Fuller Test: DGS10 Time Series')\n",
    "augmented_dickey_fuller_statistics(transform_data['DGS10'])\n",
    "print('Augmented Dickey-Fuller Test: AdjClose Time Series')\n",
    "augmented_dickey_fuller_statistics(transform_data['AdjClose'])\n",
    "\n",
    "##GoldPrice\tSilverPrice\tCrudeOilPrices(WTI)\tFEDFUNDS\tDGS10\tAdjClose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276888159
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, dpi=120, figsize=(10,6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    d = transform_data[transform_data.columns[i]]\n",
    "    ax.plot(d, color='red', linewidth=1)\n",
    "    # Decorations\n",
    "    ax.set_title(dataset.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines['top'].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger’s Causality Test: ###\n",
    "\n",
    "\n",
    "The formal definition of Granger causality can be explained as, whether past values of x aid in the prediction of yt, conditional on having already accounted for the effects on yt of past values of y (and perhaps of past values of other variables). If they do, the x is said to “Granger cause” y. So, the basis behind VAR is that each of the time series in the system influences each other.\n",
    "Granger’s causality Tests the null hypothesis that the coefficients of past values in the regression equation is zero. So, if the p-value obtained from the test is lesser than the significance level of 0.05, then, you can safely reject the null hypothesis. This has been performed on original data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276891875
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as sm\n",
    "maxlag=12\n",
    "test = 'ssr-chi2test'\n",
    "def grangers_causality_matrix(X_train, variables, test = 'ssr_chi2test', verbose=False):\n",
    "    dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in dataset.columns:\n",
    "        for r in dataset.index:\n",
    "            test_result = sm.grangercausalitytests(X_train[[r,c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            dataset.loc[r,c] = min_p_value\n",
    "    dataset.columns = [var + '_x' for var in variables]\n",
    "    dataset.index = [var + '_y' for var in variables]\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276900019
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "grangers_causality_matrix(dataset, variables = dataset.columns)\n",
    "## https://rishi-a.github.io/2020/05/25/granger-causality.html\n",
    "## https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row are the response (y) and the columns are the predictor series (x).\n",
    "* If we take the value 0.0000 in (row 1, column 5), it refers to the p-value of the Granger’s Causality test for InterestRate_x causing Gold_y. The 0.0000 in (row 5, column 1) refers to the p-value of InterestRate_y causing GoldPrice_x and so on.\n",
    "* We can see that, in the case of Interest and USD variables, we cannot reject null hypothesis e.g. USD & Silver, USD & Oil. \n",
    "* Our variables of interest are Gold and Oil here. So, for Gold, all the variables cause but for USD doesn’t causes any effect on Oil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looking at the p-Values, we can assume that, except USD, all the other variables (time series) in the system are interchangeably causing each other. This justifies the VAR modeling approach for this system of multi time-series to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR model ###\n",
    "\n",
    "VAR requires stationarity of the series which means the mean to the series do not change over time (we can find this out from the plot drawn next to Augmented Dickey-Fuller Test).\n",
    "\n",
    "\n",
    "So, I will fit the VAR model on training set and then used the fitted model to forecast the next 15 observations. These forecasts will be compared against the actual present in test data. I have taken the maximum lag (15) to identify the required lags for VAR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276906715
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "mod = VAR(transform_data)\n",
    "res = mod.fit(maxlags=15, ic='aic')\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual plot ###\n",
    "\n",
    "Residual plot looks normal with constant mean throughout apart from some large fluctuation during 2009, 2011, 2014 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276921765
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "y_fitted = res.fittedvalues\n",
    "y_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276929268
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "y_fitted = res.fittedvalues\n",
    "residuals = res.resid\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(residuals, label='resid')\n",
    "plt.plot(y_fitted, label='VAR prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin-Watson Statistic ###\n",
    "The Durbin-Watson statistic will always have a value between 0 and 4. A value of 2.0 means that there is no auto-correlation detected in the sample. Values from 0 to less than 2 indicate positive auto-correlation and values from 2 to 4 indicate negative auto-correlation. A rule of thumb is that test statistic values in the range of 1.5 to 2.5 are relatively normal. Any value outside this range could be a cause for concern.\n",
    "\n",
    "###### A stock price displaying positive auto-correlation would indicate that the price yesterday has a positive correlation on the price today — so if the stock fell yesterday, it is also likely that it falls today. A stock that has a negative auto-correlation, on the other hand, has a negative influence on itself over time — so that if it fell yesterday, there is a greater likelihood it will rise today. ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276934214
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(res.resid)\n",
    "for col,val in zip(transform_data.columns, out):\n",
    "    print((col), \":\", round(val,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is no auto-correlation (2.0) exist; so, we can proceed with the forecast. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction: #### \n",
    "In order to forecast, the VAR model expects up to the lag order number of observations from the past data. This is because, the terms in the VAR model are essentially the lags of the various time series in the data-set, so we need to provide as many of the previous values as indicated by the lag order used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276938479
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get the lag order\n",
    "lag_order = res.k_ar\n",
    "print(lag_order)\n",
    "# Input data for forecasting\n",
    "input_data = transform_data.values[-lag_order:]\n",
    "print(input_data)\n",
    "# forecasting\n",
    "pred = res.forecast(y=input_data, steps=n_obs)\n",
    "pred = (pd.DataFrame(pred, index=X_test.index, columns=X_test.columns + '_pred'))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276943998
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invert the transformation: ####\n",
    "\n",
    "The forecasts are generated but it is on the scale of the training data used by the model. So, to bring it back up to its original scale, we need to de-difference it.\n",
    "The way to convert the differencing is to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number.\n",
    "This process can be reversed by adding the observation at the prior time step to the difference value. inverted(ts) = differenced(ts) + observation(ts-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276950847
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# inverting transformation\n",
    "def invert_transformation(X_train, pred):\n",
    "    forecast = pred.copy()\n",
    "    columns = X_train.columns\n",
    "    for col in columns:\n",
    "        forecast[str(col)+'_pred'] = X_train[col].iloc[-1] + forecast[str(col)+'_pred'].cumsum()\n",
    "        return forecast\n",
    "\n",
    "output = invert_transformation(X_train, pred)\n",
    "\n",
    "## https://stackoverflow.com/questions/20461165/how-to-convert-index-of-a-pandas-dataframe-into-a-column\n",
    "output['Date'] = output.index\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276957387
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276963118
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276967318
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(output['GoldPrice_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276973409
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#combining predicted and real data set\n",
    "combine = pd.concat([output['GoldPrice_pred'], X_test['GoldPrice']], axis=1)\n",
    "combine['accuracy'] = round(combine.apply(lambda row: row.GoldPrice_pred /row.GoldPrice *100, axis = 1),2)\n",
    "combine['accuracy'] = pd.Series([\"{0:.2f}%\".format(val) for val in combine['accuracy']],index = combine.index)\n",
    "combine = combine.round(decimals=2)\n",
    "combine = combine.reset_index()\n",
    "combine = combine.sort_values(by='Date', ascending=False)\n",
    "\n",
    "print(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "#combining predicted and real data set\n",
    "combine = pd.concat([output['GoldPrice_pred'], X_test['GoldPrice']], axis=1)\n",
    "combine['accuracy'] = round(combine.apply(lambda row: row.GoldPrice_pred /row.GoldPrice *100, axis = 1),2)\n",
    "combine['accuracy'] = pd.Series([\"{0:.2f}%\".format(val) for val in combine['accuracy']],index = combine.index)\n",
    "combine = combine.round(decimals=2)\n",
    "combine = combine.reset_index()\n",
    "combine = combine.sort_values(by='Date', ascending=False)\n",
    "\n",
    "print(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: #### \n",
    "To evaluate the forecasts, a comprehensive set of metrics, such as the MAPE, ME, MAE, MPE and RMSE can be computed. We have computed some of these as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615276983474
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "#Forecast bias\n",
    "forecast_errors = [combine['GoldPrice'][i]- combine['GoldPrice_pred'][i] for i in range(len(combine['GoldPrice']))]\n",
    "bias = sum(forecast_errors) * 1.0/len(combine['GoldPrice'])\n",
    "\n",
    "print('Bias: %f' % bias) \n",
    "print('Mean absolute error:', mean_absolute_error(combine['GoldPrice'].values, combine['GoldPrice_pred'].values))\n",
    "print('Mean squared error:', mean_squared_error(combine['GoldPrice'].values, combine['GoldPrice_pred'].values))\n",
    "print('Root mean squared error:', math.sqrt(mean_squared_error(combine['GoldPrice'].values, combine['GoldPrice_pred'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615277041472
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "## https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\n",
    "\n",
    "fig, axes = plt.subplots(nrows=int(len(dataset.columns)/2), ncols=2, dpi=150, figsize=(10,10))\n",
    "for i, (col,ax) in enumerate(zip(dataset.columns, axes.flatten())):\n",
    "    output[col+'_pred'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    X_test[col][-n_obs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary #### \n",
    "\n",
    "The VAR model is a popular tool for the purpose of predicting joint dynamics of multiple time series based on linear functions of past observations. More analysis e.g. impulse response (IRF) and forecast error variance decomposition (FEVD) can also be done along-with VAR to for assessing the impacts of shock from one asset on another to assess the impacts of shock from one asset on another. However, I will keep this simple here for easy understanding. In real-life business case, we should do multiple models with different approach to do the comparative analysis before zeroed down on one or a hybrid model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet In Action ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series\n",
    "# https://dev.socrata.com/blog/2019/10/07/time-series-analysis-with-jupyter-notebooks-and-socrata.html\n",
    "\n",
    "# Data Source\n",
    "# Building Permits\n",
    "# https://data.seattle.gov/Permitting/Building-Permits/76t5-zqzr\n",
    "\n",
    "dataset.info()\n",
    "df3=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "model = Prophet()\n",
    "train_df = df3.rename(columns={\"GoldPrice\":'y'})\n",
    "train_df[\"ds\"] = train_df.index\n",
    "model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "# We want to forecast over the next 5 months\n",
    "future = model.make_future_dataframe(30, freq='D', include_history=True)\n",
    "forecast = model.predict(future)\n",
    "model.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "05d640e67ef859aab651a7bfc6c87f65a8f829a36299ad7739491cc59bf3ef8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
